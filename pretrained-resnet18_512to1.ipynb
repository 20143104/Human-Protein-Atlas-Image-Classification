{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from fastai.conv_learner import *\n",
    "from fastai.dataset import *\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import scipy.optimize as opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "1dcb0ed47f29e7a457cc54500b97ab4e96405e8e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PATH = './'\n",
    "TRAIN = '../input/train/'\n",
    "TEST = '../input/test/'\n",
    "LABELS = '../input/train.csv'\n",
    "SAMPLE = '../input/sample_submission.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "87ad005d93033f87ab77acf72d2d6f6cb4a33193",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "name_label_dict = {\n",
    "0:  'Nucleoplasm',\n",
    "1:  'Nuclear membrane',\n",
    "2:  'Nucleoli',   \n",
    "3:  'Nucleoli fibrillar center',\n",
    "4:  'Nuclear speckles',\n",
    "5:  'Nuclear bodies',\n",
    "6:  'Endoplasmic reticulum',   \n",
    "7:  'Golgi apparatus',\n",
    "8:  'Peroxisomes',\n",
    "9:  'Endosomes',\n",
    "10:  'Lysosomes',\n",
    "11:  'Intermediate filaments',\n",
    "12:  'Actin filaments',\n",
    "13:  'Focal adhesion sites',   \n",
    "14:  'Microtubules',\n",
    "15:  'Microtubule ends',  \n",
    "16:  'Cytokinetic bridge',   \n",
    "17:  'Mitotic spindle',\n",
    "18:  'Microtubule organizing center',  \n",
    "19:  'Centrosome',\n",
    "20:  'Lipid droplets',\n",
    "21:  'Plasma membrane',   \n",
    "22:  'Cell junctions', \n",
    "23:  'Mitochondria',\n",
    "24:  'Aggresome',\n",
    "25:  'Cytosol',\n",
    "26:  'Cytoplasmic bodies',   \n",
    "27:  'Rods & rings' }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8147c0b2f23cf4b646e83f3026698f6df243e468",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nw = 2   #Numwork에 필요한 변수 입니다.\n",
    "arch = resnet18 #사용할 신경망입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "13dd9234ce2f843d5d7169d5ebef515543a632bd",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_names = list({f[:36] for f in os.listdir(TRAIN)})\n",
    "\n",
    "test_names = list({f[:36] for f in os.listdir(TEST)})\n",
    "\n",
    "tr_n, val_n = train_test_split(train_names, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b5ae50160b85e023518ee17d62e4ff216eb5b9fb",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def open_rgby(path,id): #a function that reads RGBY image\n",
    "    colors = ['red','green','blue','yellow']\n",
    "    \n",
    "    flags = cv2.IMREAD_GRAYSCALE\n",
    "    \n",
    "    img = [cv2.imread(os.path.join(path, id+'_'+color+'.png'), flags).astype(np.float32)/255\n",
    "           for color in colors]\n",
    "    \n",
    "    return np.stack(img, axis=-1)\n",
    "# 사진을 읽어와서 배열로 만드는 코드 부분입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0dd72024e421ac2ced8a21389e15e914fae8d6a8",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class pdFilesDataset(FilesDataset):\n",
    "    def __init__(self, fnames, path, transform):\n",
    "        self.labels = pd.read_csv(LABELS).set_index('Id')\n",
    "        self.labels['Target'] = [[int(i) for i in s.split()] for s in self.labels['Target']]\n",
    "        super().__init__(fnames, transform, path)\n",
    "    \n",
    "    def get_x(self, i):\n",
    "        img = open_rgby(self.path,self.fnames[i])\n",
    "        if self.sz == 512: \n",
    "            return img \n",
    "        else: \n",
    "            return cv2.resize(img, (self.sz, self.sz),cv2.INTER_AREA)\n",
    "    \n",
    "    def get_y(self, i):\n",
    "        if(self.path == TEST): return np.zeros(len(name_label_dict),dtype=np.int)\n",
    "        else:\n",
    "            labels = self.labels.loc[self.fnames[i]]['Target']\n",
    "            return np.eye(len(name_label_dict),dtype=np.float)[labels].sum(axis=0) #np.eye() --> 대각선이 1인 행렬 만드는 과정\n",
    "        \n",
    "    @property\n",
    "    def is_multi(self): return True\n",
    "    @property\n",
    "    def is_reg(self):return True\n",
    "    \n",
    "    def get_c(self): return len(name_label_dict) #number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2279cbaea32d80e9bf030b4a3e74ade811a52b3a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data(sz,bs):\n",
    "    #data augmentation\n",
    "    aug_tfms = [RandomRotate(30, tfm_y=TfmType.NO),\n",
    "                RandomDihedral(tfm_y=TfmType.NO), #Random horizental과 같은 의미의 코드 입니다.\n",
    "                RandomLighting(0.05, 0.05, tfm_y=TfmType.NO)] #사진의 밝이를 조절하는 부분입니다.\n",
    "    #mean and std in of each channel in the train set\n",
    "    stats = A([0.08069, 0.05258, 0.05487, 0.08282], [0.13704, 0.10145, 0.15313, 0.13814]) #normalize를 할 때 사용할 코드 부분입니다.\n",
    "    tfms = tfms_from_stats(stats, sz, crop_type=CropType.NO, tfm_y=TfmType.NO, aug_tfms=aug_tfms)\n",
    "    ds = ImageData.get_ds(pdFilesDataset, (tr_n[:-(len(tr_n)%bs)],TRAIN), (val_n,TRAIN), tfms, test=(test_names,TEST))\n",
    "    md = ImageData(PATH, ds, bs, num_workers=nw, classes=None)\n",
    "    return md\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "24adecf3a1b558b05026ec43e64b8848ac0b05d7",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bs = 16  #batch_size\n",
    "sz = 256 #사진 size 조절을 위한 변수\n",
    "md = get_data(sz,bs)\n",
    "\n",
    "x,y = next(iter(md.trn_dl))\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a907d00a3863cc5f92a9a204cb52188466d363f3",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_tot = np.zeros(4)\n",
    "x2_tot = np.zeros(4)\n",
    "for x,y in iter(md.trn_dl):\n",
    "    tmp =  md.trn_ds.denorm(x).reshape(16,-1)\n",
    "    x = md.trn_ds.denorm(x).reshape(-1,4)\n",
    "    x_tot += x.mean(axis=0)\n",
    "    x2_tot += (x**2).mean(axis=0)\n",
    "\n",
    "channel_avr = x_tot/len(md.trn_dl)\n",
    "channel_std = np.sqrt(x2_tot/len(md.trn_dl) - channel_avr**2)\n",
    "channel_avr,channel_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b3f8762045c4087ae9aecd22f8c13a075f567d7a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def forward(self, input, target):\n",
    "        if not (target.size() == input.size()):\n",
    "            raise ValueError(\"Target size ({}) must be the same as input size ({})\"\n",
    "                             .format(target.size(), input.size()))\n",
    "\n",
    "        max_val = (-input).clamp(min=0)\n",
    "        loss = input - input * target + max_val + \\ ((-max_val).exp() + (-input - max_val).exp()).log()\n",
    "\n",
    "        invprobs = F.logsigmoid(-input * (target * 2.0 - 1.0))\n",
    "        \n",
    "        loss = (invprobs * self.gamma).exp() * loss\n",
    "        \n",
    "        return loss.sum(dim=1).mean()\n",
    "    # 현재 풀어야할 문제는 mult-classification을 하는 문제이며, 이 문제를 풀때 유용한 손실 함수가 FocalLoss입니다.\n",
    "    # 학습을 시킬때 각각의 단백질의 갯수가 다르기 때문에, 단백질의 수가 적은 것은 학습이 잘 안될 수 있습니다.\n",
    "    # Focal Loss는 cross Entropy를 기반으로 한 함수이며, cross entropy앞에 (1-p)라는 계를 이용하여 분류를 잘하는 target value\n",
    "    # 의 loss값을 작게 만드는 행위를 해서, 학습이 덜 된 Target에 좀 더 집중할 수 있게 만들어 줍니다.\n",
    "    # 이렇게 빈도수에 따른 학습량의 차이를 고려하여, backpropagation을 할 수 있게 해주는 손실함수가 FocalLoss입니다.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "06c8f2af90481b71e1f174c584b7d1d2a0697363",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def acc(preds,targs,th=0.0):\n",
    "    preds = (preds > th).int()\n",
    "    targs = targs.int()\n",
    "    return (preds==targs).float().mean()\n",
    "# 정확도를 측정하는 함수입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "96778d73cae9d6a9a3f1acc2fecd1a7f5363e29b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ConvnetBuilder_custom():\n",
    "    def __init__(self, f, c, is_multi, is_reg, ps=None, xtra_fc=None, xtra_cut=0, custom_head=None, pretrained=True):\n",
    "        self.f, self.c, self.is_multi, self.is_reg, self.xtra_cut = f, c, is_multi, is_reg, xtra_cut\n",
    "        if xtra_fc is None: xtra_fc = [512]\n",
    "            \n",
    "        if ps is None: ps = [0.25]*len(xtra_fc) + [0.5]\n",
    "            \n",
    "        self.ps,self.xtra_fc = ps,xtra_fc\n",
    "\n",
    "        if f in model_meta: cut, self.lr_cut = model_meta[f]\n",
    "        \n",
    "        else: cut,self.lr_cut = 0,0\n",
    "        \n",
    "        cut-=xtra_cut\n",
    "        \n",
    "        layers = cut_model(f(pretrained), cut)\n",
    "        \n",
    "        # 현재 이 신경망의 구현은 transfer-learning을 한것입니다. \n",
    "        # imagenet을 이용하여 학습된 것에, 현재 우리의 도메인이 yellow가 존재하기 때문에 신경망에 하나의 층을 추가하는 부분의 코드입니다.\n",
    "        w = layers[0].weight\n",
    "        \n",
    "        layers[0] = nn.Conv2d(4,64,kernel_size=(7,7),stride=(2,2),padding=(3, 3), bias=False)\n",
    "        \n",
    "        layers[0].weight = torch.nn.Parameter(torch.cat((w,torch.zeros(64,1,7,7)),dim=1))\n",
    "        # concat을 할때 dim = 0 이면 아래로 concat을 하고 dim = 1을 하면 옆으로 붙있다.\n",
    "        # 즉 row를 늘리는 방향 ==> dim = 0 이며, col를 늘리는 방향 ==> dim = 1을 해주면 된다.\n",
    "        # 신경망의 input 부분을 늘려주는 행위를 하기 때문에 col방향으로 concat을 해주어야 합니다.\n",
    "        \n",
    "        self.nf = model_features[f] if f in model_features else (num_features(layers)*2)\n",
    "        \n",
    "        if not custom_head: layers += [AdaptiveConcatPool2d(), Flatten()]\n",
    "        #  Flatten() 은 fully connected layer를 추가해 주는 기능을 합니다.\n",
    "        \n",
    "        self.top_model = nn.Sequential(*layers)\n",
    "\n",
    "        n_fc = len(self.xtra_fc)+1\n",
    "        # fully_connected layer 를 만들기 위해 필요한 변수\n",
    "        \n",
    "        if not isinstance(self.ps, list): self.ps = [self.ps]*n_fc\n",
    "        \n",
    "        if custom_head: \n",
    "            fc_layers = [custom_head]\n",
    "        else: \n",
    "            fc_layers = self.get_fc_layers()\n",
    "        \n",
    "        self.n_fc = len(fc_layers)\n",
    "        \n",
    "        self.fc_model = to_gpu(nn.Sequential(*fc_layers))\n",
    "        \n",
    "        if not custom_head: \n",
    "            apply_init(self.fc_model, kaiming_normal)\n",
    "        \n",
    "        self.model = to_gpu(nn.Sequential(*(layers+fc_layers)))\n",
    "\n",
    "    @property\n",
    "    def name(self): return f'{self.f.__name__}_{self.xtra_cut}'\n",
    "\n",
    "    def create_fc_layer(self, ni, nf, p, actn=None):\n",
    "        res=[nn.BatchNorm1d(num_features=ni)]\n",
    "        if p:\n",
    "            res.append(nn.Dropout(p=p))\n",
    "        \n",
    "        res.append(nn.Linear(in_features=ni, out_features=nf))\n",
    "        \n",
    "        if actn:\n",
    "            res.append(actn)\n",
    "        \n",
    "        return res\n",
    "\n",
    "    def get_fc_layers(self):\n",
    "        res=[]         \n",
    "        ni=self.nf\n",
    "        for i,nf in enumerate(self.xtra_fc):\n",
    "            res += self.create_fc_layer(ni, nf, p=self.ps[i], actn=nn.ReLU())\n",
    "            ni=nf\n",
    "        final_actn = nn.Sigmoid() if self.is_multi else nn.LogSoftmax()            \n",
    "        if self.is_reg: \n",
    "            final_actn = None\n",
    "        \n",
    "        res += self.create_fc_layer(ni, self.c, p=self.ps[-1], actn=final_actn)\n",
    "        \n",
    "        return res\n",
    "\n",
    "    def get_layer_groups(self, do_fc=False):\n",
    "        if do_fc: return [self.fc_model]\n",
    "        \n",
    "        idxs = [self.lr_cut]\n",
    "        \n",
    "        c = children(self.top_model)\n",
    "        \n",
    "        if len(c)==3: \n",
    "            c = children(c[0])+c[1:]\n",
    "        lgs = list(split_by_idxs(c,idxs))\n",
    "        return lgs+[self.fc_model]\n",
    "    \n",
    "class ConvLearner(Learner):\n",
    "    def __init__(self, data, models, precompute=False, **kwargs):\n",
    "        self.precompute = False\n",
    "        \n",
    "        super().__init__(data, models, **kwargs)\n",
    "        \n",
    "        if hasattr(data, 'is_multi') and not data.is_reg and self.metrics is None:\n",
    "            self.metrics = [accuracy_thresh(0.5)] if self.data.is_multi else [accuracy]\n",
    "        \n",
    "        if precompute: self.save_fc1()\n",
    "            \n",
    "        self.freeze()\n",
    "        \n",
    "        self.precompute = precompute\n",
    "\n",
    "    def _get_crit(self, data):\n",
    "        \n",
    "        if not hasattr(data, 'is_multi'): return super()._get_crit(data)\n",
    "\n",
    "        return F.l1_loss if data.is_reg else F.binary_cross_entropy if data.is_multi else F.nll_loss\n",
    "\n",
    "    @classmethod\n",
    "    def pretrained(cls, f, data, ps=None, xtra_fc=None, xtra_cut=0, custom_head=None, precompute=False,\n",
    "                   pretrained=True, **kwargs):\n",
    "        models = ConvnetBuilder_custom(f, data.c, data.is_multi, data.is_reg,\n",
    "            ps=ps, xtra_fc=xtra_fc, xtra_cut=xtra_cut, custom_head=custom_head, pretrained=pretrained)\n",
    "        return cls(data, models, precompute, **kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def lsuv_learner(cls, f, data, ps=None, xtra_fc=None, xtra_cut=0, custom_head=None, precompute=False,\n",
    "                  needed_std=1.0, std_tol=0.1, max_attempts=10, do_orthonorm=False, **kwargs):\n",
    "        models = ConvnetBuilder(f, data.c, data.is_multi, data.is_reg,\n",
    "            ps=ps, xtra_fc=xtra_fc, xtra_cut=xtra_cut, custom_head=custom_head, pretrained=False)\n",
    "        convlearn=cls(data, models, precompute, **kwargs)\n",
    "        convlearn.lsuv_init()\n",
    "        return convlearn\n",
    "    \n",
    "    @property\n",
    "    def model(self): return self.models.fc_model if self.precompute else self.models.model\n",
    "    \n",
    "    def half(self):\n",
    "        if self.fp16: return\n",
    "        self.fp16 = True\n",
    "        if type(self.model) != FP16: self.models.model = FP16(self.model)\n",
    "        if not isinstance(self.models.fc_model, FP16): self.models.fc_model = FP16(self.models.fc_model)\n",
    "    def float(self):\n",
    "        if not self.fp16: return\n",
    "        self.fp16 = False\n",
    "        if type(self.models.model) == FP16: self.models.model = self.model.module.float()\n",
    "        if type(self.models.fc_model) == FP16: self.models.fc_model = self.models.fc_model.module.float()\n",
    "\n",
    "    @property\n",
    "    def data(self): return self.fc_data if self.precompute else self.data_\n",
    "\n",
    "    def create_empty_bcolz(self, n, name):\n",
    "        return bcolz.carray(np.zeros((0,n), np.float32), chunklen=1, mode='w', rootdir=name)\n",
    "\n",
    "    def set_data(self, data, precompute=False):\n",
    "        super().set_data(data)\n",
    "        if precompute:\n",
    "            self.unfreeze()\n",
    "            self.save_fc1()\n",
    "            self.freeze()\n",
    "            self.precompute = True\n",
    "        else:\n",
    "            self.freeze()\n",
    "\n",
    "    def get_layer_groups(self):\n",
    "        return self.models.get_layer_groups(self.precompute)\n",
    "\n",
    "    def summary(self):\n",
    "        precompute = self.precompute\n",
    "        self.precompute = False\n",
    "        res = super().summary()\n",
    "        self.precompute = precompute\n",
    "        return res\n",
    "\n",
    "    def get_activations(self, force=False):\n",
    "        tmpl = f'_{self.models.name}_{self.data.sz}.bc'\n",
    "        names = [os.path.join(self.tmp_path, p+tmpl) for p in ('x_act', 'x_act_val', 'x_act_test')]\n",
    "        if os.path.exists(names[0]) and not force:\n",
    "            self.activations = [bcolz.open(p) for p in names]\n",
    "        else:\n",
    "            self.activations = [self.create_empty_bcolz(self.models.nf,n) for n in names]\n",
    "\n",
    "    def save_fc1(self):\n",
    "        self.get_activations()\n",
    "        act, val_act, test_act = self.activations\n",
    "        m=self.models.top_model\n",
    "        if len(self.activations[0])!=len(self.data.trn_ds):\n",
    "            predict_to_bcolz(m, self.data.fix_dl, act)\n",
    "        if len(self.activations[1])!=len(self.data.val_ds):\n",
    "            predict_to_bcolz(m, self.data.val_dl, val_act)\n",
    "        if self.data.test_dl and (len(self.activations[2])!=len(self.data.test_ds)):\n",
    "            if self.data.test_dl: predict_to_bcolz(m, self.data.test_dl, test_act)\n",
    "\n",
    "        self.fc_data = ImageClassifierData.from_arrays(self.data.path,\n",
    "                (act, self.data.trn_y), (val_act, self.data.val_y), self.data.bs, classes=self.data.classes,\n",
    "                test = test_act if self.data.test_dl else None, num_workers=8)\n",
    "\n",
    "    def freeze(self):\n",
    "        self.freeze_to(-1)\n",
    "\n",
    "    def unfreeze(self):\n",
    "        self.freeze_to(0)\n",
    "        self.precompute = False\n",
    "\n",
    "    def predict_array(self, arr):\n",
    "        precompute = self.precompute\n",
    "        self.precompute = False\n",
    "        pred = super().predict_array(arr)\n",
    "        self.precompute = precompute\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "17c6374e2043d19b460ae425eb4009855423bc27",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sz = 512 #image size\n",
    "bs = 64  #batch size\n",
    "\n",
    "md = get_data(sz,bs)\n",
    "learner = ConvLearner.pretrained(arch, md, ps=0.5) #dropout 50%\n",
    "learner.opt_fn = optim.Adam\n",
    "learner.clip = 1.0 #gradient clipping\n",
    "learner.crit = FocalLoss()\n",
    "learner.metrics = [acc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5ecad6cfc5289308523f2d7b946c1926c9b7a1c9",
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learner.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "61cb0f0ceb2141d4bd3d2f2b181a207f7b503dd2",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.lr_find()\n",
    "learner.sched.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fa183a7b7809e4cbd6aa16916d9915a24826d58b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = 2e-2\n",
    "learner.fit(lr,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "032d2c930d002704e5690f095fc0af043c739982",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.unfreeze()\n",
    "lrs=np.array([lr/10,lr/3,lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7b01fe75583db3453bc3712e4c7d9e36f2e5bdb0",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.fit(lrs/4,4,cycle_len=2,use_clr=(10,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f36c0b2468425ee0f064decbc1ee40777c84eb52",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.fit(lrs/4,2,cycle_len=4,use_clr=(10,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3753ac42996bbdff557eddb8778ae5b6e3d613af",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.sched.plot_lr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "775ed9f50c239c18f063ae9d385ad0f951ac68fa",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.fit(lrs/16,1,cycle_len=8,use_clr=(5,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "032a66e6f9db41870dbbbfa5ce7a106363f8b8e0",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.save('ResNet34_256_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0e19235b01078762646c74bfe41e0862305d8740"
   },
   "source": [
    "### Validation score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e5f8481c905db44b4132967adec66d70521aa06b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid_np(x):\n",
    "    return 1.0/(1.0 + np.exp(-x))\n",
    "\n",
    "preds,y = learner.TTA(n_aug=16)\n",
    "preds = np.stack(preds, axis=-1)\n",
    "preds = sigmoid_np(preds)\n",
    "pred = preds.max(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fb4c482fc261eabd13e367305f7d6583d1e4d45f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def F1_soft(preds,targs,th=0.5,d=50.0):\n",
    "    preds = sigmoid_np(d*(preds - th))\n",
    "    targs = targs.astype(np.float)\n",
    "    score = 2.0*(preds*targs).sum(axis=0)/((preds+targs).sum(axis=0) + 1e-6)\n",
    "    return score\n",
    "\n",
    "def fit_val(x,y):\n",
    "    params = 0.5*np.ones(len(name_label_dict))\n",
    "    wd = 1e-5\n",
    "    error = lambda p: np.concatenate((F1_soft(x,y,p) - 1.0,\n",
    "                                      wd*(p - 0.5)), axis=None)\n",
    "    p, success = opt.leastsq(error, params)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "98cadeb22bcdbf5ddde44a98fa9e90d68249741a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "th = fit_val(pred,y)\n",
    "th[th<0.1] = 0.1\n",
    "print('Thresholds: ',th)\n",
    "print('F1 macro: ',f1_score(y, pred>th, average='macro'))\n",
    "print('F1 macro (th = 0.5): ',f1_score(y, pred>0.5, average='macro'))\n",
    "print('F1 micro: ',f1_score(y, pred>th, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8ca60fed703d8ce6fe9d524359bfae7f4feb7882",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Fractions: ',(pred > th).mean(axis=0))\n",
    "print('Fractions (true): ',(y > th).mean(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "83132c40a82e38beb33ba62fa19e5a0233a9cbbb"
   },
   "source": [
    "### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cedaa4bf73c9b3381ac3e6da32b040e05ce24b91",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds_t,y_t = learner.TTA(n_aug=16,is_test=True)\n",
    "preds_t = np.stack(preds_t, axis=-1)\n",
    "preds_t = sigmoid_np(preds_t)\n",
    "pred_t = preds_t.max(axis=-1) #max works better for F1 macro score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2cd07338fab7fcd9b883e01ea44ec8a55d42d60c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_pred(pred, th=0.5, fname='protein_classification.csv'):\n",
    "    pred_list = []\n",
    "    for line in pred:\n",
    "        s = ' '.join(list([str(i) for i in np.nonzero(line>th)[0]]))\n",
    "        pred_list.append(s)\n",
    "        \n",
    "    sample_df = pd.read_csv(SAMPLE)\n",
    "    sample_list = list(sample_df.Id)\n",
    "    pred_dic = dict((key, value) for (key, value) \n",
    "                in zip(learner.data.test_ds.fnames,pred_list))\n",
    "    pred_list_cor = [pred_dic[id] for id in sample_list]\n",
    "    df = pd.DataFrame({'Id':sample_list,'Predicted':pred_list_cor})\n",
    "    df.to_csv(fname, header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a1c388b048f1d3f613c1d93d2ea8259be25c54ec",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "th_t = np.array([0.565,0.39,0.55,0.345,0.33,0.39,0.33,0.45,0.38,0.39,\n",
    "               0.34,0.42,0.31,0.38,0.49,0.50,0.38,0.43,0.46,0.40,\n",
    "               0.39,0.505,0.37,0.47,0.41,0.545,0.32,0.1])\n",
    "print('Fractions: ',(pred_t > th_t).mean(axis=0))\n",
    "save_pred(pred_t,th_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "546164c9576241139af6de50e5c3b585fcee2bbd",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lb_prob = [\n",
    " 0.362397820,0.043841336,0.075268817,0.059322034,0.075268817,\n",
    " 0.075268817,0.043841336,0.075268817,0.010000000,0.010000000,\n",
    " 0.010000000,0.043841336,0.043841336,0.014198783,0.043841336,\n",
    " 0.010000000,0.028806584,0.014198783,0.028806584,0.059322034,\n",
    " 0.010000000,0.126126126,0.028806584,0.075268817,0.010000000,\n",
    " 0.222493880,0.028806584,0.010000000]\n",
    "# I replaced 0 by 0.01 since there may be a rounding error leading to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "966d9e3a03f689369df5ce3e3c74c3d6bda28c29",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Count_soft(preds,th=0.5,d=50.0):\n",
    "    preds = sigmoid_np(d*(preds - th))\n",
    "    return preds.mean(axis=0)\n",
    "\n",
    "def fit_test(x,y):\n",
    "    params = 0.5*np.ones(len(name_label_dict))\n",
    "    wd = 1e-5\n",
    "    error = lambda p: np.concatenate((Count_soft(x,p) - y,\n",
    "                                      wd*(p - 0.5)), axis=None)\n",
    "    p, success = opt.leastsq(error, params)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "25b3c95265890a290fb640ad4b4498e13d81881d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "th_t = fit_test(pred_t,lb_prob)\n",
    "th_t[th_t<0.1] = 0.1\n",
    "print('Thresholds: ',th_t)\n",
    "print('Fractions: ',(pred_t > th_t).mean(axis=0))\n",
    "print('Fractions (th = 0.5): ',(pred_t > 0.5).mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e613772527bca8a6aa9f4bd9f7b89575966b2947",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_pred(pred_t,th_t,'protein_classification_f.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1f822ae26a507eb79986de38234ed10f10f9190f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_pred(pred_t,th,'protein_classification_v.csv')\n",
    "save_pred(pred_t,0.5,'protein_classification_05.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6371d4a8ad55d3b0b0d757ebd42b0ebba7efefc4",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class_list = [8,9,10,15,20,24,27]\n",
    "for i in class_list:\n",
    "    th_t[i] = th[i]\n",
    "save_pred(pred_t,th_t,'protein_classification_c.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "effda6d101ee0ce6c160da72be7f869df4f09d07",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = pd.read_csv(LABELS).set_index('Id')\n",
    "label_count = np.zeros(len(name_label_dict))\n",
    "for label in labels['Target']:\n",
    "    l = [int(i) for i in label.split()]\n",
    "    label_count += np.eye(len(name_label_dict))[l].sum(axis=0)\n",
    "label_fraction = label_count.astype(np.float)/len(labels)\n",
    "label_count, label_fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "083b552fae17124170dbb7dcc673e89b805f97fb",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "th_t = fit_test(pred_t,label_fraction)\n",
    "th_t[th_t<0.05] = 0.05\n",
    "print('Thresholds: ',th_t)\n",
    "print('Fractions: ',(pred_t > th_t).mean(axis=0))\n",
    "save_pred(pred_t,th_t,'protein_classification_t.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
